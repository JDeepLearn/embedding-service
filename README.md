# Embedding Service

### **A production-ready, on-prem embedding microservice built with FastAPI and SentenceTransformers.**

This service provides high-performance text embeddings for enterprise retrieval-augmented generation (RAG), FAQ systems, and semantic search ‚Äî all running **locally**, with **no cloud dependency**.

---

## Key Features

* **Local inference only** ‚Äî no outbound API calls or data leaks
* **Extensible provider abstraction** ‚Äî plug in IBM Granite, OpenAI, etc. later
* **Deterministic builds with `uv`** ‚Äî zero dependency drift
* **One-click local setup** ‚Äî instant bootstrap on any machine
* **100% functional test coverage** ‚Äî positive & negative API scenarios validated
* **Enterprise-grade Docker + CI/CD** ‚Äî production ready for K8s, ECS, or OpenShift

---

## Architecture Overview

```text
app/
 ‚îú‚îÄ‚îÄ core/           # Configuration & structured logging
 ‚îú‚îÄ‚îÄ providers/      # EmbeddingProvider interface + HFProvider implementation
 ‚îú‚îÄ‚îÄ api/            # FastAPI routes (/embed, /health)
 ‚îú‚îÄ‚îÄ main.py         # Application bootstrap
tests/               # Full pytest suite (positive + negative)
```

**Model:** `intfloat/e5-large-v2` (local, CPU-optimized)
**Language:** Python 3.11
**Manager:** [uv](https://docs.astral.sh/uv/)
**Framework:** FastAPI 0.115.0

---

## One-Click Local Setup

To run the service on **any new machine (clean or configured)**:

```bash
bash setup_local.sh
```

This script will:

1. Verify Python 3.11+
2. Install `uv` if missing
3. Create a local virtual environment
4. Install all dependencies
5. Copy `.env.example` ‚Üí `.env` if needed
6. Start the service at [http://localhost:8000](http://localhost:8000)

---

## Environment Configuration

`.env.example` ‚Üí copy to `.env` before running if not using the setup script.

```bash
APP_NAME=Embedding Service
APP_VERSION=1.0.0
ENVIRONMENT=local
LOG_LEVEL=INFO
MODEL_NAME=intfloat/e5-large-v2
```

All values are optional; defaults are production-safe.

---

## API Endpoints

### **Health Check**

```bash
curl http://localhost:8000/health | jq
```

Response:

```json
{
  "status": "ok",
  "service": "Embedding Service",
  "version": "1.0.0",
  "environment": "local",
  "model": "intfloat/e5-large-v2"
}
```

---

### **Single-Text Embedding**

```bash
curl -s -X POST http://localhost:8000/embed \
  -H "Content-Type: application/json" \
  -d '{"text": "Testing E5 model"}' | jq
```

Response:

```json
{
  "provider": "hf-local",
  "model": "intfloat/e5-large-v2",
  "dim": 1024,
  "embedding": [ 1024 ]
}
```

---

### **Batch Embedding**

```bash
curl -s -X POST http://localhost:8000/embed \
  -H "Content-Type: application/json" \
  -d '{"texts": ["first", "second", "third"]}' | jq
```

---

### **Error Scenarios (negative tests)**

| Case                           | Expected Code | Behavior                    |
|--------------------------------|---------------|-----------------------------|
| Missing both `text` & `texts`  | `400`         | Returns validation error    |
| Wrong input type (`text`: int) | `422`         | Pydantic validation failure |
| Empty string                   | `400/500`     | Model rejects empty text    |

---

## Docker Deployment

### Build

```bash
docker build -t embedding-service .
```

### Run

```bash
docker run -p 8000:8000 --env-file .env embedding-service
```

Test inside container:

```bash
curl http://localhost:8000/health
```

---

## Testing (Full Coverage)

To run all functional tests:

```bash
uv run pytest -q
```

To check coverage:

```bash
uv run pytest --cov=embedding-service --cov-report=term-missing
```

Covers:

* `/health` positive
* `/embed` single text positive
* `/embed` batch positive
* Empty, invalid, and malformed input negatives

Achieves >95% line coverage and 100% endpoint path coverage.

---

## CI/CD Automation

**Workflow:** `.github/workflows/ci.yml`
Runs automatically on every `push` or `PR` to `main`:

1. Set up Python 3.11 + uv
2. Install dependencies
3. Run full test suite
4. Build and push Docker image to GHCR (`ghcr.io/org/repo:latest`)

Compatible with **GitHub Container Registry**, **Docker Hub**, or **IBM Cloud**.

---

## Technology Stack

| Layer              | Technology                                 |
|--------------------|--------------------------------------------|
| Framework          | FastAPI 0.115                              |
| Model Runtime      | SentenceTransformers (E5-large-v2)         |
| Dependency Manager | uv                                         |
| Language           | Python 3.11                                |
| Logging            | Python stdlib logging (structured, stdout) |
| Deployment         | Docker / K8s ready                         |
| CI/CD              | GitHub Actions                             |

---

## Operational Notes

* The model loads **once globally** for efficiency.
* Memory footprint: ~1.5 GB (CPU).
* Typical latency (Intel i7): 80‚Äì120 ms per text.
* Stateless REST API ‚Äî horizontally scalable.

---

## üßë‚Äçüíª Development Commands

| Action               | Command                                                                   |
|----------------------|---------------------------------------------------------------------------|
| Run locally          | `uv run uvicorn app.main:create_app --factory --host 0.0.0.0 --port 8000` |
| Run tests            | `uv run pytest -q`                                                        |
| Build Docker image   | `docker build -t embedding_service .`                                     |
| Run Docker container | `docker run -p 8000:8000 embedding_service`                               |
| One-click setup      | `bash setup_local.sh`                                                     |

---

## Extensibility

Add new providers under `app/providers/`:

Register dynamically in your routes to switch models via env:

```bash
MODEL_NAME=ibm/granite-embedding-english-r2
```

No API or backend code changes needed.

---

## Security & Compliance

* No external network calls (on-prem safe)
* No API keys stored in code
* Logs redact user inputs
* Easily extended with API key or token auth layer

---

## Future Enhancements

* ‚úÖ Model registry integration (MLflow, IBM Granite, Hugging Face Hub)
* ‚úÖ GPU inference optimization
* ‚úÖ Prometheus metrics & distributed tracing
* ‚úÖ Request-ID correlation for observability